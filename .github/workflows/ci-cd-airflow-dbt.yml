name: CI/CD for Airflow, Python, and dbt

on:
  push:
    branches:
      - master
  pull_request:
    branches:
      - master

jobs:
  setup:
    name: Setup Environment
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.8"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

  python-tests:
    name: Run Python Tests
    runs-on: ubuntu-latest
    needs: setup

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.8"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Python Unit Tests
        run: |
          pytest tests/

  dbt-tests:
    name: Run dbt Tests
    runs-on: ubuntu-latest
    needs: setup

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Debug Directory Structure
        run: |
          pwd
          ls -R

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.8"

      - name: Install dbt
        run: |
          pip install dbt-core dbt-postgres  # Replace with your dbt adapter

      - name: Run dbt Commands
        working-directory: ./dags/data_warehouse
        env:
          DBT_PROFILES_DIR: ~/.dbt
        run: |
          dbt deps
          dbt seed --full-refresh
          dbt run
          dbt test

  deploy-airflow:
    name: Deploy Airflow DAGs
    runs-on: ubuntu-latest
    needs:
      - setup
      - python-tests
      - dbt-tests

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Sync DAGs to Airflow Server
        env:
          AIRFLOW_SSH_KEY: ${{ secrets.AIRFLOW_SSH_KEY }}
          AIRFLOW_HOST: ${{ secrets.AIRFLOW_HOST }}
          AIRFLOW_USER: ${{ secrets.AIRFLOW_USER }}
        run: |
          mkdir -p ~/.ssh
          echo "$AIRFLOW_SSH_KEY" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          rsync -avz --exclude='*.pyc' --exclude='__pycache__/' dags/ $AIRFLOW_USER@$AIRFLOW_HOST:/path/to/airflow/dags/

  notify:
    name: Notify Success or Failure
    runs-on: ubuntu-latest
    needs:
      - python-tests
      - dbt-tests
      - deploy-airflow

    steps:
      - name: Send Notification
        if: always()
        run: |
          echo "Job Status: ${{ job.status }}"
          # Add notification logic, e.g., Slack or email integration
